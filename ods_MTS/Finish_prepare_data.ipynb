{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d5b9a4-2854-4e16-bc11-7870d5533d81",
   "metadata": {
    "id": "c0d5b9a4-2854-4e16-bc11-7870d5533d81"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2f683c-f810-4ee0-8612-3dc235e46e30",
   "metadata": {
    "id": "9f2f683c-f810-4ee0-8612-3dc235e46e30"
   },
   "outputs": [],
   "source": [
    "#Константные списки\n",
    "part_days = ['evening', 'night', 'day', 'morning']\n",
    "week_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday', 'Saturday', 'Sunday' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac28d61-5d6b-4bb5-8b33-4a348abb33ab",
   "metadata": {
    "id": "aac28d61-5d6b-4bb5-8b33-4a348abb33ab"
   },
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8b7df-c0e9-4d8d-ab98-0183acf34ceb",
   "metadata": {
    "id": "b3f8b7df-c0e9-4d8d-ab98-0183acf34ceb"
   },
   "source": [
    "### Считываем данные с признаками последовательно из отдельных паркетных файлов и сразу же агрегируем их по user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641ce972-ec3b-4e34-943b-61b915664e19",
   "metadata": {
    "id": "641ce972-ec3b-4e34-943b-61b915664e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START creating in 23:48:50\n",
      "Data file parcet_0.Data read. Used 16.886138200759888 sec.\n",
      "Data file parcet_0.Week days calc. Used 13.027215480804443 sec.\n",
      "Data file parcet_0.reparing strings finished. Used 85.56792116165161 sec.\n",
      "Data file parcet_0.Grouping finished. Prom users df create. Used 249.3321213722229 sec.\n",
      "Data file parcet_0. Deleting dublicates finished. Used 16.442820072174072 sec.\n",
      "Data file parcet_0.Grouping finished. All_users upgrate. Used 0.00099945068359375 sec.\n",
      "Data file parcet_0.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_1.Data read. Used 17.030155897140503 sec.\n",
      "Data file parcet_1.Week days calc. Used 13.055615663528442 sec.\n",
      "Data file parcet_1.reparing strings finished. Used 82.79020857810974 sec.\n",
      "Data file parcet_1.Grouping finished. Prom users df create. Used 246.77850675582886 sec.\n",
      "Data file parcet_1. Deleting dublicates finished. Used 16.604637384414673 sec.\n",
      "Data file parcet_1.Grouping finished. All_users upgrate. Used 1.6166026592254639 sec.\n",
      "Data file parcet_1.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_2.Data read. Used 16.250959634780884 sec.\n",
      "Data file parcet_2.Week days calc. Used 13.002044916152954 sec.\n",
      "Data file parcet_2.reparing strings finished. Used 83.71755456924438 sec.\n",
      "Data file parcet_2.Grouping finished. Prom users df create. Used 259.5620241165161 sec.\n",
      "Data file parcet_2. Deleting dublicates finished. Used 16.259339570999146 sec.\n",
      "Data file parcet_2.Grouping finished. All_users upgrate. Used 0.8855068683624268 sec.\n",
      "Data file parcet_2.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_3.Data read. Used 16.276628494262695 sec.\n",
      "Data file parcet_3.Week days calc. Used 12.708593606948853 sec.\n",
      "Data file parcet_3.reparing strings finished. Used 163.0139000415802 sec.\n",
      "Data file parcet_3.Grouping finished. Prom users df create. Used 342.0290033817291 sec.\n",
      "Data file parcet_3. Deleting dublicates finished. Used 16.54709768295288 sec.\n",
      "Data file parcet_3.Grouping finished. All_users upgrate. Used 0.29889988899230957 sec.\n",
      "Data file parcet_3.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_4.Data read. Used 16.635054349899292 sec.\n",
      "Data file parcet_4.Week days calc. Used 12.719804763793945 sec.\n",
      "Data file parcet_4.reparing strings finished. Used 104.02192163467407 sec.\n",
      "Data file parcet_4.Grouping finished. Prom users df create. Used 284.8153667449951 sec.\n",
      "Data file parcet_4. Deleting dublicates finished. Used 16.088839292526245 sec.\n",
      "Data file parcet_4.Grouping finished. All_users upgrate. Used 1.0332081317901611 sec.\n",
      "Data file parcet_4.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_5.Data read. Used 18.41028380393982 sec.\n",
      "Data file parcet_5.Week days calc. Used 12.834066390991211 sec.\n",
      "Data file parcet_5.reparing strings finished. Used 195.74833154678345 sec.\n",
      "Data file parcet_5.Grouping finished. Prom users df create. Used 290.2990336418152 sec.\n",
      "Data file parcet_5. Deleting dublicates finished. Used 17.242295026779175 sec.\n",
      "Data file parcet_5.Grouping finished. All_users upgrate. Used 1.7249979972839355 sec.\n",
      "Data file parcet_5.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_6.Data read. Used 18.782997608184814 sec.\n",
      "Data file parcet_6.Week days calc. Used 12.694751024246216 sec.\n",
      "Data file parcet_6.reparing strings finished. Used 87.70224332809448 sec.\n",
      "Data file parcet_6.Grouping finished. Prom users df create. Used 311.5879158973694 sec.\n",
      "Data file parcet_6. Deleting dublicates finished. Used 17.49433970451355 sec.\n",
      "Data file parcet_6.Grouping finished. All_users upgrate. Used 3.0153651237487793 sec.\n",
      "Data file parcet_6.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_7.Data read. Used 20.334166288375854 sec.\n",
      "Data file parcet_7.Week days calc. Used 12.708986043930054 sec.\n",
      "Data file parcet_7.reparing strings finished. Used 90.38188076019287 sec.\n",
      "Data file parcet_7.Grouping finished. Prom users df create. Used 306.5441539287567 sec.\n",
      "Data file parcet_7. Deleting dublicates finished. Used 17.115580320358276 sec.\n",
      "Data file parcet_7.Grouping finished. All_users upgrate. Used 6.388761281967163 sec.\n",
      "Data file parcet_7.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_8.Data read. Used 17.198750972747803 sec.\n",
      "Data file parcet_8.Week days calc. Used 12.72175407409668 sec.\n",
      "Data file parcet_8.reparing strings finished. Used 144.2865571975708 sec.\n",
      "Data file parcet_8.Grouping finished. Prom users df create. Used 318.1729872226715 sec.\n",
      "Data file parcet_8. Deleting dublicates finished. Used 19.440390825271606 sec.\n",
      "Data file parcet_8.Grouping finished. All_users upgrate. Used 24.64652705192566 sec.\n",
      "Data file parcet_8.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "Data file parcet_9.Data read. Used 24.304768562316895 sec.\n",
      "Data file parcet_9.Week days calc. Used 12.761467933654785 sec.\n",
      "Data file parcet_9.reparing strings finished. Used 94.02791976928711 sec.\n",
      "Data file parcet_9.Grouping finished. Prom users df create. Used 367.0791115760803 sec.\n",
      "Data file parcet_9. Deleting dublicates finished. Used 19.258195400238037 sec.\n",
      "Data file parcet_9.Grouping finished. All_users upgrate. Used 9.647391557693481 sec.\n",
      "Data file parcet_9.Grouping finished. Csv-file upgrate. Used 0.0 sec.\n",
      "FINISHED creating in 01:06:10. Summery used time 0.0 sec.\n"
     ]
    }
   ],
   "source": [
    "#Data with features\n",
    "#Creating DF with agregate information from all files\n",
    "first_all_users = 0\n",
    "top_urls = []\n",
    "data_files = ['parcet_0', 'parcet_1', 'parcet_2', 'parcet_3', 'parcet_4', 'parcet_5', 'parcet_6', 'parcet_7', 'parcet_8', 'parcet_9']\n",
    "\n",
    "\n",
    "START = time.time()\n",
    "print(f'START creating in {time.strftime(\"%H:%M:%S\", time.localtime())}')\n",
    "for data_file in data_files:\n",
    "    start = time.time()\n",
    "    data = pd.read_parquet(data_file)\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.Data read. Used {use_time} sec.\")\n",
    "    \n",
    "    start = time.time()\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['week_day'] = data.date.dt.day_name()\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.Week days calc. Used {use_time} sec.\")\n",
    "   \n",
    "  \n",
    "    start = time.time()\n",
    "    data['region_name'] = data.region_name.transform(lambda x: '_'.join(x.split(\" \")))\n",
    "    data['city_name'] = data.city_name.transform(lambda x: '_'.join(x.split(\" \")))\n",
    "    data['cpe_model_name'] = data.cpe_model_name.transform(lambda x: '_'.join(x.split(\" \")))\n",
    "    data['cpe_type_cd'] = data['cpe_type_cd'] + ' ' \n",
    "    data['region_name'] = data['region_name'] + ' ' \n",
    "    data['city_name'] = data['city_name'] + ' ' \n",
    "    data['cpe_model_name'] = data['cpe_model_name'] + ' '\n",
    "    data['week_day'] = data['week_day'] + ' ' \n",
    "    data['week_day'] = data['week_day'] *data['request_cnt']\n",
    "    data['part_of_day'] = data['part_of_day'] + ' '\n",
    "    data['url_host'] = data['part_of_day'] + data['url_host'] + ' '\n",
    "    data['part_of_day'] = data['part_of_day']*data['request_cnt']\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.reparing strings finished. Used {use_time} sec.\")\n",
    "     \n",
    "    start = time.time()\n",
    "    users = data.groupby(by = 'user_id', as_index = False).agg(\\\n",
    "                                                           cpe_type_cd = ('cpe_type_cd', sum), region_name = ('region_name', sum), \\\n",
    "                                                           city_name = ('city_name', sum), cpe_model_name = ('cpe_model_name', sum),\\\n",
    "                                                          begin_date = ('date', min), end_date = ('date', max), week_days = ('week_day', sum),\\\n",
    "                                                           part_of_day = ('part_of_day', sum), url_host = ('url_host', sum), price = ('price', 'mean'))\n",
    "    \n",
    "    del data\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.Grouping finished. Prom users df create. Used {use_time} sec.\")\n",
    "    \n",
    "    start = time.time()\n",
    "    users['region_name'] = users.region_name.transform(lambda x: ' '.join(set(x.split(\" \"))))\n",
    "    users['city_name'] = users.city_name.transform(lambda x: ' '.join(set(x.split(\" \"))))   \n",
    "    users['cpe_model_name'] = users.cpe_model_name.transform(lambda x: ' '.join(set(x.split(\" \")))) \n",
    "    users['cpe_type_cd'] = users.cpe_type_cd.transform(lambda x: ' '.join(set(x.split(\" \"))))\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}. Deleting dublicates finished. Used {use_time} sec.\")\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    if first_all_users == 0:\n",
    "        all_users = users\n",
    "        first_all_users = 1\n",
    "    else:\n",
    "        all_users = pd.concat([all_users, users])\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.Grouping finished. All_users upgrate. Used {use_time} sec.\") \n",
    "    \n",
    "    del users\n",
    "    \n",
    "    start = time.time()\n",
    "    #all_users.to_csv('all_users')\n",
    "    end = time.time()\n",
    "    use_time = (end - start)\n",
    "    print(f\"Data file {data_file}.Grouping finished. Csv-file upgrate. Used {use_time} sec.\") \n",
    "\n",
    "\n",
    "END = time.time()\n",
    "use_time = (end - start)   \n",
    "print(f'FINISHED creating in {time.strftime(\"%H:%M:%S\", time.localtime())}. Summery used time {use_time} sec.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db164a1-5d71-4fce-83fa-1a4704b0faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id               0\n",
       "cpe_type_cd           0\n",
       "region_name           0\n",
       "city_name             0\n",
       "cpe_model_name        0\n",
       "begin_date            0\n",
       "end_date              0\n",
       "week_days             0\n",
       "part_of_day           0\n",
       "url_host              0\n",
       "price             10703\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6b32c-bf9d-4503-aeaa-1850d51b53da",
   "metadata": {
    "id": "6ee6b32c-bf9d-4503-aeaa-1850d51b53da"
   },
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63b026e-f7c9-4691-966f-6bc340e67d92",
   "metadata": {
    "id": "d63b026e-f7c9-4691-966f-6bc340e67d92"
   },
   "outputs": [],
   "source": [
    "#Calculation statistics for user_id in aggregate DF\n",
    "#Сount rows for each part of day for each user_id. Write in new columns ('evening', 'night', 'day', 'morning')\n",
    "for part in part_days:\n",
    "    all_users[part] = all_users.part_of_day.transform(lambda x: Counter(str(x).split())[part]) \n",
    "\n",
    "#Сount rows for each day of week for each user_id. Write in new columns ('Monday', 'Tuesday', 'Wednesday', 'Thursday','Friday', 'Saturday', 'Sunday')    \n",
    "for day in week_days:\n",
    "    all_users[day] = all_users.week_days.transform(lambda x: Counter(str(x).split())[day]) \n",
    "\n",
    "#Calc most frequency town, region and model cpe for each user_id. Write in a new columns ('city', 'region', 'cpe').\n",
    "all_users['city']  = all_users['city_name'].transform(lambda x: Counter(str(x).split()).most_common(1)[0][0])   \n",
    "all_users['region']  = all_users['region_name'].transform(lambda x: Counter(str(x).split()).most_common(1)[0][0]) \n",
    "all_users['cpe']  = all_users['cpe_model_name'].transform(lambda x: Counter(str(x).split()).most_common(1)[0][0])  \n",
    "\n",
    "#Calc sum count of citys, regions and models cpe for each user_id. Write in a new columns ('city_count', 'region_count').\n",
    "all_users['city_count']  = all_users['city_name'].transform(lambda x: len(Counter(str(x).split()))) \n",
    "all_users['region_count']  = all_users['region_name'].transform(lambda x: len(Counter(str(x).split())))\n",
    "all_users['cpe_count']  = all_users['cpe_model_name'].transform(lambda x: len(Counter(str(x).split()))) \n",
    "    \n",
    "#Drop unusefull columns    \n",
    "all_users = all_users.drop(['part_of_day', 'week_days'], axis = 1)\n",
    "all_users = all_users.drop(['region_name','city_name'], axis = 1)\n",
    "all_users = all_users.drop(['cpe_model_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f018373f-9195-463e-b989-29725869b02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id             0\n",
       "cpe_type_cd         0\n",
       "begin_date          0\n",
       "end_date            0\n",
       "url_host            0\n",
       "price           10703\n",
       "evening             0\n",
       "night               0\n",
       "day                 0\n",
       "morning             0\n",
       "Monday              0\n",
       "Tuesday             0\n",
       "Wednesday           0\n",
       "Thursday            0\n",
       "Friday              0\n",
       "Saturday            0\n",
       "Sunday              0\n",
       "city                0\n",
       "region              0\n",
       "cpe                 0\n",
       "city_count          0\n",
       "region_count        0\n",
       "cpe_count           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be93b1f9-51e5-4ba0-b02b-c0624c34a4b3",
   "metadata": {
    "id": "be93b1f9-51e5-4ba0-b02b-c0624c34a4b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save prepared data ibn a file\n",
    "all_users.to_csv('all_data_prepared_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d025430a-b48a-423e-89e8-4d52d5519cd6",
   "metadata": {
    "id": "d025430a-b48a-423e-89e8-4d52d5519cd6"
   },
   "outputs": [],
   "source": [
    "#Clear memory \n",
    "del all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c890ac4-b94d-4246-923e-1d7e2fac953e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
